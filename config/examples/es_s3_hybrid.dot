# Elasticsearch / S3 Hybrid Vector Configuration Example
# ------------------------------------------------------------------------------
# This demonstrates a hybrid pipeline, writing data to both Elasticsearch and
# AWS S3. This is advantageous because each storage helps to offset its
# counterpart's weaknesses. You can provision Elasticsearch for performance
# and delegate durability to S3.

digraph {
  data_dir = "/var/lib/vector"

  subgraph sources {
    # Ingest data by tailing one or more files
    # Docs: https://vector.dev/docs/reference/sources/file
    apache_logs [
      type = "file"
      include = "/var/log/*.log"
      ignore_older = 86400 # 1 day
    ]
  }

  # Optionally parse, structure and transform data here.
  # Docs: https://vector.dev/docs/reference/transforms

  subgraph sinks {
    # Send structured data to Elasticsearch for searching of recent data
    es_cluster [
        type = "elasticsearch"
        host = "79.12.221.222:9200"
        doc_type = "_doc"
    ]

    # Send structured data to S3, a durable long-term storage
    s3_archives [
      type = "aws_s3"
      region = "us-east-1"
      bucket = "my_log_archives"
      encoding = "ndjson"
      compression = "gzip"
      "batch.max_size" = 10000000 # 10mb uncompressed
    ]
  }

  # Set up the data flow
  apache_logs -> es_cluster
  apache_logs -> s3_archives
}
