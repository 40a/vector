digraph {
  # Set global options
  data_dir = "/var/lib/vector"

  subgraph sources {
    # Ingest data by tailing one or more files
    apache_logs [
      type = "file"
      include = "/var/log/apache2/*.log" # supports globbing
      ignore_older = 86400 # 1 day
    ]
  }

  subgraph transforms {
    # Structure and parse the data
    apache_parser [
      type = "regex_parser" # fast/powerful regex
      regex = "^(?P<host>[w.]+) - (?P<user>[w]+) (?P<bytes_in>[d]+) [(?P<timestamp>.*)] \"(?P<method>[w]+) (?P<path>.*)\" (?P<status>[d]+) (?P<bytes_out>[d]+)$"
    ]

    # Sample the data to save on cost
    apache_sampler [
      type = "sampler"
      rate = 50 # only keep 50%
    ]
  }

  subgraph sinks {
    # Send structured data to a short-term storage
    es_cluster [
      type = "elasticsearch"
      host = "http://79.12.221.222:9200" # local or external host
    ]

    # Send structured data to a cost-effective long-term storage
    s3_archives [
      type = "aws_s3"
      region = "us-east-1"
      bucket = "my-log-archives"
      key_prefix = "date=%Y-%m-%d" # daily partitions, hive friendly format
      compression = "gzip" # compress final objects
      encoding = "ndjson" # new line delimited JSON

      "batch.max_size" = 10000000 # 10mb uncompressed
    ]
  }

  # Connect the components
  apache_logs -> apache_parser -> apache_sampler
  apache_sampler -> es_cluster
  apache_parser -> s3_archives
}
